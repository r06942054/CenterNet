{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/omnieyes/renjie/Github/CenterNet/src/lib')\n",
    "sys.path.append('/home/omnieyes/renjie/Github/CenterNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('resdcn_18', {'hm': 20, 'wh': 2, 'reg': 2}, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 \t\t\t\t\t\t\t\t# 批处理大小\n",
    "input_shape = (3, 512, 512) \t\t\t\t# 输入数据\n",
    "\n",
    "# set the model to inference mode\n",
    "model.eval()\n",
    "\n",
    "#x = torch.randn(batch_size,*input_shape) \t# 生成张量\n",
    "x = torch.ones(1, 3, 512, 512, requires_grad=False)\n",
    "export_onnx_file = \"resdcn_18.onnx\" \t\t\t\t# 目的ONNX文件名\n",
    "torch.onnx.export(model,\n",
    "                    x,\n",
    "                    export_onnx_file,\n",
    "                    #opset_version=10,\n",
    "#                     input_names=[\"input\"],\t\t# 输入名\n",
    "#                     output_names=[\"output\"],\t# 输出名\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/omnieyes/renjie/GitHub/CenterNet/src/lib')\n",
    "import torch\n",
    "from models.model import create_model, load_model\n",
    "from opts import opts\n",
    "from torch.autograd import Variable\n",
    "# from summary import summary\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading pretrained model https://download.pytorch.org/models/resnet18-5c106cde.pth\n"
    }
   ],
   "source": [
    "model = create_model('res_18', {'hm': 9, 'wh': 2, 'reg': 2}, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 256, 256]           9,408\n       BatchNorm2d-2         [-1, 64, 256, 256]             128\n              ReLU-3         [-1, 64, 256, 256]               0\n         MaxPool2d-4         [-1, 64, 128, 128]               0\n            Conv2d-5         [-1, 64, 128, 128]          36,864\n       BatchNorm2d-6         [-1, 64, 128, 128]             128\n              ReLU-7         [-1, 64, 128, 128]               0\n            Conv2d-8         [-1, 64, 128, 128]          36,864\n       BatchNorm2d-9         [-1, 64, 128, 128]             128\n             ReLU-10         [-1, 64, 128, 128]               0\n       BasicBlock-11         [-1, 64, 128, 128]               0\n           Conv2d-12         [-1, 64, 128, 128]          36,864\n      BatchNorm2d-13         [-1, 64, 128, 128]             128\n             ReLU-14         [-1, 64, 128, 128]               0\n           Conv2d-15         [-1, 64, 128, 128]          36,864\n      BatchNorm2d-16         [-1, 64, 128, 128]             128\n             ReLU-17         [-1, 64, 128, 128]               0\n       BasicBlock-18         [-1, 64, 128, 128]               0\n           Conv2d-19          [-1, 128, 64, 64]          73,728\n      BatchNorm2d-20          [-1, 128, 64, 64]             256\n             ReLU-21          [-1, 128, 64, 64]               0\n           Conv2d-22          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-23          [-1, 128, 64, 64]             256\n           Conv2d-24          [-1, 128, 64, 64]           8,192\n      BatchNorm2d-25          [-1, 128, 64, 64]             256\n             ReLU-26          [-1, 128, 64, 64]               0\n       BasicBlock-27          [-1, 128, 64, 64]               0\n           Conv2d-28          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-29          [-1, 128, 64, 64]             256\n             ReLU-30          [-1, 128, 64, 64]               0\n           Conv2d-31          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-32          [-1, 128, 64, 64]             256\n             ReLU-33          [-1, 128, 64, 64]               0\n       BasicBlock-34          [-1, 128, 64, 64]               0\n           Conv2d-35          [-1, 256, 32, 32]         294,912\n      BatchNorm2d-36          [-1, 256, 32, 32]             512\n             ReLU-37          [-1, 256, 32, 32]               0\n           Conv2d-38          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-39          [-1, 256, 32, 32]             512\n           Conv2d-40          [-1, 256, 32, 32]          32,768\n      BatchNorm2d-41          [-1, 256, 32, 32]             512\n             ReLU-42          [-1, 256, 32, 32]               0\n       BasicBlock-43          [-1, 256, 32, 32]               0\n           Conv2d-44          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-45          [-1, 256, 32, 32]             512\n             ReLU-46          [-1, 256, 32, 32]               0\n           Conv2d-47          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-48          [-1, 256, 32, 32]             512\n             ReLU-49          [-1, 256, 32, 32]               0\n       BasicBlock-50          [-1, 256, 32, 32]               0\n           Conv2d-51          [-1, 512, 16, 16]       1,179,648\n      BatchNorm2d-52          [-1, 512, 16, 16]           1,024\n             ReLU-53          [-1, 512, 16, 16]               0\n           Conv2d-54          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-55          [-1, 512, 16, 16]           1,024\n           Conv2d-56          [-1, 512, 16, 16]         131,072\n      BatchNorm2d-57          [-1, 512, 16, 16]           1,024\n             ReLU-58          [-1, 512, 16, 16]               0\n       BasicBlock-59          [-1, 512, 16, 16]               0\n           Conv2d-60          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-61          [-1, 512, 16, 16]           1,024\n             ReLU-62          [-1, 512, 16, 16]               0\n           Conv2d-63          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n             ReLU-65          [-1, 512, 16, 16]               0\n       BasicBlock-66          [-1, 512, 16, 16]               0\n  ConvTranspose2d-67          [-1, 256, 32, 32]       2,097,152\n      BatchNorm2d-68          [-1, 256, 32, 32]             512\n             ReLU-69          [-1, 256, 32, 32]               0\n  ConvTranspose2d-70          [-1, 256, 64, 64]       1,048,576\n      BatchNorm2d-71          [-1, 256, 64, 64]             512\n             ReLU-72          [-1, 256, 64, 64]               0\n  ConvTranspose2d-73        [-1, 256, 128, 128]       1,048,576\n      BatchNorm2d-74        [-1, 256, 128, 128]             512\n             ReLU-75        [-1, 256, 128, 128]               0\n           Conv2d-76         [-1, 64, 128, 128]         147,520\n             ReLU-77         [-1, 64, 128, 128]               0\n           Conv2d-78          [-1, 9, 128, 128]             585\n           Conv2d-79         [-1, 64, 128, 128]         147,520\n             ReLU-80         [-1, 64, 128, 128]               0\n           Conv2d-81          [-1, 2, 128, 128]             130\n           Conv2d-82         [-1, 64, 128, 128]         147,520\n             ReLU-83         [-1, 64, 128, 128]               0\n           Conv2d-84          [-1, 2, 128, 128]             130\n================================================================\nTotal params: 15,815,757\nTrainable params: 15,815,757\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 3.00\nForward/backward pass size (MB): 503.62\nParams size (MB): 60.33\nEstimated Total Size (MB): 566.96\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(model.cuda(), (3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nArgs:\n    name (string): name of the child module. The child module can be\n        accessed from this module using the given name\n    parameter (Module): child module to be added to the module.\n\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/CenterNet/lib/python3.6/site-packages/torch/nn/modules/module.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "model.add_module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading pretrained model https://download.pytorch.org/models/resnet18-5c106cde.pth\nloaded /home/omnieyes/renjie/GitHub/CenterNet/exp/ctdet/omnieyes_res_18/model_best.pth, epoch 95\n============================\nhm\nwh\nreg\ngraph(%0 : Float(1, 3, 512, 512)\n      %1 : Float(64, 3, 7, 7)\n      %2 : Float(64)\n      %3 : Float(64)\n      %4 : Float(64)\n      %5 : Float(64)\n      %6 : Long()\n      %7 : Float(64, 64, 3, 3)\n      %8 : Float(64)\n      %9 : Float(64)\n      %10 : Float(64)\n      %11 : Float(64)\n      %12 : Long()\n      %13 : Float(64, 64, 3, 3)\n      %14 : Float(64)\n      %15 : Float(64)\n      %16 : Float(64)\n      %17 : Float(64)\n      %18 : Long()\n      %19 : Float(64, 64, 3, 3)\n      %20 : Float(64)\n      %21 : Float(64)\n      %22 : Float(64)\n      %23 : Float(64)\n      %24 : Long()\n      %25 : Float(64, 64, 3, 3)\n      %26 : Float(64)\n      %27 : Float(64)\n      %28 : Float(64)\n      %29 : Float(64)\n      %30 : Long()\n      %31 : Float(128, 64, 3, 3)\n      %32 : Float(128)\n      %33 : Float(128)\n      %34 : Float(128)\n      %35 : Float(128)\n      %36 : Long()\n      %37 : Float(128, 128, 3, 3)\n      %38 : Float(128)\n      %39 : Float(128)\n      %40 : Float(128)\n      %41 : Float(128)\n      %42 : Long()\n      %43 : Float(128, 64, 1, 1)\n      %44 : Float(128)\n      %45 : Float(128)\n      %46 : Float(128)\n      %47 : Float(128)\n      %48 : Long()\n      %49 : Float(128, 128, 3, 3)\n      %50 : Float(128)\n      %51 : Float(128)\n      %52 : Float(128)\n      %53 : Float(128)\n      %54 : Long()\n      %55 : Float(128, 128, 3, 3)\n      %56 : Float(128)\n      %57 : Float(128)\n      %58 : Float(128)\n      %59 : Float(128)\n      %60 : Long()\n      %61 : Float(256, 128, 3, 3)\n      %62 : Float(256)\n      %63 : Float(256)\n      %64 : Float(256)\n      %65 : Float(256)\n      %66 : Long()\n      %67 : Float(256, 256, 3, 3)\n      %68 : Float(256)\n      %69 : Float(256)\n      %70 : Float(256)\n      %71 : Float(256)\n      %72 : Long()\n      %73 : Float(256, 128, 1, 1)\n      %74 : Float(256)\n      %75 : Float(256)\n      %76 : Float(256)\n      %77 : Float(256)\n      %78 : Long()\n      %79 : Float(256, 256, 3, 3)\n      %80 : Float(256)\n      %81 : Float(256)\n      %82 : Float(256)\n      %83 : Float(256)\n      %84 : Long()\n      %85 : Float(256, 256, 3, 3)\n      %86 : Float(256)\n      %87 : Float(256)\n      %88 : Float(256)\n      %89 : Float(256)\n      %90 : Long()\n      %91 : Float(512, 256, 3, 3)\n      %92 : Float(512)\n      %93 : Float(512)\n      %94 : Float(512)\n      %95 : Float(512)\n      %96 : Long()\n      %97 : Float(512, 512, 3, 3)\n      %98 : Float(512)\n      %99 : Float(512)\n      %100 : Float(512)\n      %101 : Float(512)\n      %102 : Long()\n      %103 : Float(512, 256, 1, 1)\n      %104 : Float(512)\n      %105 : Float(512)\n      %106 : Float(512)\n      %107 : Float(512)\n      %108 : Long()\n      %109 : Float(512, 512, 3, 3)\n      %110 : Float(512)\n      %111 : Float(512)\n      %112 : Float(512)\n      %113 : Float(512)\n      %114 : Long()\n      %115 : Float(512, 512, 3, 3)\n      %116 : Float(512)\n      %117 : Float(512)\n      %118 : Float(512)\n      %119 : Float(512)\n      %120 : Long()\n      %121 : Float(512, 256, 4, 4)\n      %122 : Float(256)\n      %123 : Float(256)\n      %124 : Float(256)\n      %125 : Float(256)\n      %126 : Long()\n      %127 : Float(256, 256, 4, 4)\n      %128 : Float(256)\n      %129 : Float(256)\n      %130 : Float(256)\n      %131 : Float(256)\n      %132 : Long()\n      %133 : Float(256, 256, 4, 4)\n      %134 : Float(256)\n      %135 : Float(256)\n      %136 : Float(256)\n      %137 : Float(256)\n      %138 : Long()\n      %139 : Float(64, 256, 3, 3)\n      %140 : Float(64)\n      %141 : Float(9, 64, 1, 1)\n      %142 : Float(9)\n      %143 : Float(64, 256, 3, 3)\n      %144 : Float(64)\n      %145 : Float(2, 64, 1, 1)\n      %146 : Float(2)\n      %147 : Float(64, 256, 3, 3)\n      %148 : Float(64)\n      %149 : Float(2, 64, 1, 1)\n      %150 : Float(2)) {\n  %151 : Float(1, 64, 256, 256) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%0, %1), scope: PoseResNet/Conv2d[conv1]\n  %152 : Float(1, 64, 256, 256) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%151, %2, %3, %4, %5), scope: PoseResNet/BatchNorm2d[bn1]\n  %153 : Float(1, 64, 256, 256) = onnx::Relu(%152), scope: PoseResNet/ReLU[relu]\n  %154 : Float(1, 64, 128, 128) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%153), scope: PoseResNet/MaxPool2d[maxpool]\n  %155 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%154, %7), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1]\n  %156 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%155, %8, %9, %10, %11), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1]\n  %157 : Float(1, 64, 128, 128) = onnx::Relu(%156), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n  %158 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%157, %13), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2]\n  %159 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%158, %14, %15, %16, %17), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]\n  %160 : Float(1, 64, 128, 128) = onnx::Add(%159, %154), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]\n  %161 : Float(1, 64, 128, 128) = onnx::Relu(%160), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n  %162 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%161, %19), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1]\n  %163 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%162, %20, %21, %22, %23), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn1]\n  %164 : Float(1, 64, 128, 128) = onnx::Relu(%163), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n  %165 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%164, %25), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv2]\n  %166 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%165, %26, %27, %28, %29), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]\n  %167 : Float(1, 64, 128, 128) = onnx::Add(%166, %161), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]\n  %168 : Float(1, 64, 128, 128) = onnx::Relu(%167), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n  %169 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%168, %31), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1]\n  %170 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%169, %32, %33, %34, %35), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1]\n  %171 : Float(1, 128, 64, 64) = onnx::Relu(%170), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n  %172 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%171, %37), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2]\n  %173 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%172, %38, %39, %40, %41), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]\n  %174 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%168, %43), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n  %175 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%174, %44, %45, %46, %47), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n  %176 : Float(1, 128, 64, 64) = onnx::Add(%173, %175), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]\n  %177 : Float(1, 128, 64, 64) = onnx::Relu(%176), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n  %178 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%177, %49), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv1]\n  %179 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%178, %50, %51, %52, %53), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn1]\n  %180 : Float(1, 128, 64, 64) = onnx::Relu(%179), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n  %181 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%180, %55), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv2]\n  %182 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%181, %56, %57, %58, %59), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]\n  %183 : Float(1, 128, 64, 64) = onnx::Add(%182, %177), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]\n  %184 : Float(1, 128, 64, 64) = onnx::Relu(%183), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n  %185 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%184, %61), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1]\n  %186 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%185, %62, %63, %64, %65), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1]\n  %187 : Float(1, 256, 32, 32) = onnx::Relu(%186), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n  %188 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%187, %67), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2]\n  %189 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%188, %68, %69, %70, %71), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]\n  %190 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%184, %73), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n  %191 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%190, %74, %75, %76, %77), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n  %192 : Float(1, 256, 32, 32) = onnx::Add(%189, %191), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]\n  %193 : Float(1, 256, 32, 32) = onnx::Relu(%192), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n  %194 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%193, %79), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv1]\n  %195 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%194, %80, %81, %82, %83), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn1]\n  %196 : Float(1, 256, 32, 32) = onnx::Relu(%195), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n  %197 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%196, %85), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv2]\n  %198 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%197, %86, %87, %88, %89), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]\n  %199 : Float(1, 256, 32, 32) = onnx::Add(%198, %193), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]\n  %200 : Float(1, 256, 32, 32) = onnx::Relu(%199), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n  %201 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%200, %91), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1]\n  %202 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%201, %92, %93, %94, %95), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn1]\n  %203 : Float(1, 512, 16, 16) = onnx::Relu(%202), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n  %204 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%203, %97), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2]\n  %205 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%204, %98, %99, %100, %101), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]\n  %206 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%200, %103), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n  %207 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%206, %104, %105, %106, %107), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n  %208 : Float(1, 512, 16, 16) = onnx::Add(%205, %207), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]\n  %209 : Float(1, 512, 16, 16) = onnx::Relu(%208), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n  %210 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%209, %109), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv1]\n  %211 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%210, %110, %111, %112, %113), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn1]\n  %212 : Float(1, 512, 16, 16) = onnx::Relu(%211), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n  %213 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%212, %115), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv2]\n  %214 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%213, %116, %117, %118, %119), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]\n  %215 : Float(1, 512, 16, 16) = onnx::Add(%214, %209), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]\n  %216 : Float(1, 512, 16, 16) = onnx::Relu(%215), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n  %217 : Float(1, 256, 32, 32) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%216, %121), scope: PoseResNet/Sequential[deconv_layers]/ConvTranspose2d[0]\n  %218 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%217, %122, %123, %124, %125), scope: PoseResNet/Sequential[deconv_layers]/BatchNorm2d[1]\n  %219 : Float(1, 256, 32, 32) = onnx::Relu(%218), scope: PoseResNet/Sequential[deconv_layers]/ReLU[2]\n  %220 : Float(1, 256, 64, 64) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%219, %127), scope: PoseResNet/Sequential[deconv_layers]/ConvTranspose2d[3]\n  %221 : Float(1, 256, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%220, %128, %129, %130, %131), scope: PoseResNet/Sequential[deconv_layers]/BatchNorm2d[4]\n  %222 : Float(1, 256, 64, 64) = onnx::Relu(%221), scope: PoseResNet/Sequential[deconv_layers]/ReLU[5]\n  %223 : Float(1, 256, 128, 128) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%222, %133), scope: PoseResNet/Sequential[deconv_layers]/ConvTranspose2d[6]\n  %224 : Float(1, 256, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%223, %134, %135, %136, %137), scope: PoseResNet/Sequential[deconv_layers]/BatchNorm2d[7]\n  %225 : Float(1, 256, 128, 128) = onnx::Relu(%224), scope: PoseResNet/Sequential[deconv_layers]/ReLU[8]\n  %226 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%225, %139, %140), scope: PoseResNet/Sequential[hm]/Conv2d[0]\n  %227 : Float(1, 64, 128, 128) = onnx::Relu(%226), scope: PoseResNet/Sequential[hm]/ReLU[1]\n  %228 : Float(1, 9, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%227, %141, %142), scope: PoseResNet/Sequential[hm]/Conv2d[2]\n  %229 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%225, %147, %148), scope: PoseResNet/Sequential[wh]/Conv2d[0]\n  %230 : Float(1, 64, 128, 128) = onnx::Relu(%229), scope: PoseResNet/Sequential[wh]/ReLU[1]\n  %231 : Float(1, 2, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%230, %149, %150), scope: PoseResNet/Sequential[wh]/Conv2d[2]\n  %232 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%225, %143, %144), scope: PoseResNet/Sequential[reg]/Conv2d[0]\n  %233 : Float(1, 64, 128, 128) = onnx::Relu(%232), scope: PoseResNet/Sequential[reg]/ReLU[1]\n  %234 : Float(1, 2, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%233, %145, %146), scope: PoseResNet/Sequential[reg]/Conv2d[2]\n  return (%228, %231, %234);\n}\n\n"
    }
   ],
   "source": [
    "model = create_model('res_18', {'hm': 9, 'wh': 2, 'reg': 2}, 64)\n",
    "model = load_model(model, '/home/omnieyes/renjie/GitHub/CenterNet/exp/ctdet/omnieyes_res_18/model_best.pth')\n",
    "# model = model.cuda()\n",
    "# summary(model.cuda(), (3,512,512))\n",
    "# print(model)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "#         #input()\n",
    "\n",
    "torch_model = model\n",
    "batch_size = 1\n",
    "input_shape = (3, 512, 512)\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model = torch_model\n",
    "torch_model.eval()\n",
    "\n",
    "\n",
    "dummy_input = Variable(torch.randn(1,3,512,512))\n",
    "export_onnx_file = \"/home/omnieyes/renjie/GitHub/CenterNet/exp/ctdet/omnieyes_res_18/model_best.onnx\" \n",
    "torch.onnx.export(torch_model,\n",
    "                    dummy_input,\n",
    "                    export_onnx_file,\n",
    "                    #keep_initializers_as_inputs=True,\n",
    "#                     opset_version=10,\n",
    "#                     do_constant_folding=True,\t# 是否执行常量折叠优化\n",
    "#                     input_names=[\"input\"],\t\t# 输入名\n",
    "#                     output_names=[\"output\"],\t# 输出名\n",
    "\n",
    "                    verbose=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "============================\nhm\nwh\nreg\n"
    }
   ],
   "source": [
    "result = model.forward(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "BN_MOMENTUM = 0.1\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n",
    "                                  momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PoseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, heads, head_conv, **kwargs):\n",
    "        self.inplanes = 64\n",
    "        self.deconv_with_bias = False\n",
    "        self.heads = heads\n",
    "\n",
    "        super(PoseResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # used for deconv layers\n",
    "        self.deconv_layers = self._make_deconv_layer(\n",
    "            3,\n",
    "            [256, 256, 256],\n",
    "            [4, 4, 4],\n",
    "        )\n",
    "        # self.final_layer = []\n",
    "\n",
    "        for head in sorted(self.heads):\n",
    "          num_output = self.heads[head]\n",
    "          if head_conv > 0:\n",
    "            if head == 'hm':\n",
    "                fc = nn.Sequential(\n",
    "                    nn.Conv2d(256, head_conv,\n",
    "                    kernel_size=3, padding=1, bias=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(head_conv, num_output, \n",
    "                    kernel_size=1, stride=1, padding=0),\n",
    "                    nn.Sigmoid()\n",
    "                    )\n",
    "            else:\n",
    "                fc = nn.Sequential(\n",
    "                    nn.Conv2d(256, head_conv,\n",
    "                    kernel_size=3, padding=1, bias=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(head_conv, num_output, \n",
    "                    kernel_size=1, stride=1, padding=0)\n",
    "                    )\n",
    "          else:\n",
    "            fc = nn.Conv2d(\n",
    "              in_channels=256,\n",
    "              out_channels=num_output,\n",
    "              kernel_size=1,\n",
    "              stride=1,\n",
    "              padding=0\n",
    "          )\n",
    "          self.__setattr__(head, fc)\n",
    "\n",
    "        # self.final_layer = nn.ModuleList(self.final_layer)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _get_deconv_cfg(self, deconv_kernel, index):\n",
    "        if deconv_kernel == 4:\n",
    "            padding = 1\n",
    "            output_padding = 0\n",
    "        elif deconv_kernel == 3:\n",
    "            padding = 1\n",
    "            output_padding = 1\n",
    "        elif deconv_kernel == 2:\n",
    "            padding = 0\n",
    "            output_padding = 0\n",
    "\n",
    "        return deconv_kernel, padding, output_padding\n",
    "\n",
    "    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n",
    "        assert num_layers == len(num_filters), \\\n",
    "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
    "        assert num_layers == len(num_kernels), \\\n",
    "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            kernel, padding, output_padding = \\\n",
    "                self._get_deconv_cfg(num_kernels[i], i)\n",
    "\n",
    "            planes = num_filters[i]\n",
    "            layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=self.inplanes,\n",
    "                    out_channels=planes,\n",
    "                    kernel_size=kernel,\n",
    "                    stride=2,\n",
    "                    padding=padding,\n",
    "                    output_padding=output_padding,\n",
    "                    bias=self.deconv_with_bias))\n",
    "            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            self.inplanes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.deconv_layers(x)\n",
    "        # ret = {}\n",
    "        # for head in self.heads:\n",
    "        #     ret[head] = self.__getattr__(head)(x)\n",
    "        # return [ret]\n",
    "        ret = []\n",
    "        # ret = [hm, wh, reg]\n",
    "        for head in self.heads:\n",
    "            ret.append(self.__getattr__(head)(x))\n",
    "        return [ret]\n",
    "\n",
    "    def init_weights(self, num_layers, pretrained=True):\n",
    "        if pretrained:\n",
    "            # print('=> init resnet deconv weights from normal distribution')\n",
    "            for _, m in self.deconv_layers.named_modules():\n",
    "                if isinstance(m, nn.ConvTranspose2d):\n",
    "                    # print('=> init {}.weight as normal(0, 0.001)'.format(name))\n",
    "                    # print('=> init {}.bias as 0'.format(name))\n",
    "                    nn.init.normal_(m.weight, std=0.001)\n",
    "                    if self.deconv_with_bias:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    # print('=> init {}.weight as 1'.format(name))\n",
    "                    # print('=> init {}.bias as 0'.format(name))\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # print('=> init final conv weights from normal distribution')\n",
    "            for head in self.heads:\n",
    "              final_layer = self.__getattr__(head)\n",
    "              for i, m in enumerate(final_layer.modules()):\n",
    "                  if isinstance(m, nn.Conv2d):\n",
    "                      # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                      # print('=> init {}.weight as normal(0, 0.001)'.format(name))\n",
    "                      # print('=> init {}.bias as 0'.format(name))\n",
    "                      if m.weight.shape[0] == self.heads[head]:\n",
    "                          if 'hm' in head:\n",
    "                              nn.init.constant_(m.bias, -2.19)\n",
    "                          else:\n",
    "                              nn.init.normal_(m.weight, std=0.001)\n",
    "                              nn.init.constant_(m.bias, 0)\n",
    "            #pretrained_state_dict = torch.load(pretrained)\n",
    "            url = model_urls['resnet{}'.format(num_layers)]\n",
    "            pretrained_state_dict = model_zoo.load_url(url)\n",
    "            print('=> loading pretrained model {}'.format(url))\n",
    "            self.load_state_dict(pretrained_state_dict, strict=False)\n",
    "        else:\n",
    "            print('=> imagenet pretrained model dose not exist')\n",
    "            print('=> please download it first')\n",
    "            raise ValueError('imagenet pretrained model does not exist')\n",
    "\n",
    "\n",
    "resnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),\n",
    "               34: (BasicBlock, [3, 4, 6, 3]),\n",
    "               50: (Bottleneck, [3, 4, 6, 3]),\n",
    "               101: (Bottleneck, [3, 4, 23, 3]),\n",
    "               152: (Bottleneck, [3, 8, 36, 3])}\n",
    "\n",
    "\n",
    "def get_pose_net(num_layers, heads, head_conv):\n",
    "  block_class, layers = resnet_spec[num_layers]\n",
    "\n",
    "  model = PoseResNet(block_class, layers, heads, head_conv=head_conv)\n",
    "  model.init_weights(num_layers, pretrained=True)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> loading pretrained model https://download.pytorch.org/models/resnet18-5c106cde.pth\nloaded /home/omnieyes/renjie/GitHub/CenterNet/exp/ctdet/omnieyes_res_18/model_best.pth, epoch 95\n"
    }
   ],
   "source": [
    "model = get_pose_net(num_layers=18, heads={'hm': 9, 'wh': 2, 'reg': 2}, head_conv=64)\n",
    "model = load_model(model, '/home/omnieyes/renjie/GitHub/CenterNet/exp/ctdet/omnieyes_res_18/model_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "graph(%0 : Float(1, 3, 512, 512)\n      %1 : Float(64, 3, 7, 7)\n      %2 : Float(64)\n      %3 : Float(64)\n      %4 : Float(64)\n      %5 : Float(64)\n      %6 : Long()\n      %7 : Float(64, 64, 3, 3)\n      %8 : Float(64)\n      %9 : Float(64)\n      %10 : Float(64)\n      %11 : Float(64)\n      %12 : Long()\n      %13 : Float(64, 64, 3, 3)\n      %14 : Float(64)\n      %15 : Float(64)\n      %16 : Float(64)\n      %17 : Float(64)\n      %18 : Long()\n      %19 : Float(64, 64, 3, 3)\n      %20 : Float(64)\n      %21 : Float(64)\n      %22 : Float(64)\n      %23 : Float(64)\n      %24 : Long()\n      %25 : Float(64, 64, 3, 3)\n      %26 : Float(64)\n      %27 : Float(64)\n      %28 : Float(64)\n      %29 : Float(64)\n      %30 : Long()\n      %31 : Float(128, 64, 3, 3)\n      %32 : Float(128)\n      %33 : Float(128)\n      %34 : Float(128)\n      %35 : Float(128)\n      %36 : Long()\n      %37 : Float(128, 128, 3, 3)\n      %38 : Float(128)\n      %39 : Float(128)\n      %40 : Float(128)\n      %41 : Float(128)\n      %42 : Long()\n      %43 : Float(128, 64, 1, 1)\n      %44 : Float(128)\n      %45 : Float(128)\n      %46 : Float(128)\n      %47 : Float(128)\n      %48 : Long()\n      %49 : Float(128, 128, 3, 3)\n      %50 : Float(128)\n      %51 : Float(128)\n      %52 : Float(128)\n      %53 : Float(128)\n      %54 : Long()\n      %55 : Float(128, 128, 3, 3)\n      %56 : Float(128)\n      %57 : Float(128)\n      %58 : Float(128)\n      %59 : Float(128)\n      %60 : Long()\n      %61 : Float(256, 128, 3, 3)\n      %62 : Float(256)\n      %63 : Float(256)\n      %64 : Float(256)\n      %65 : Float(256)\n      %66 : Long()\n      %67 : Float(256, 256, 3, 3)\n      %68 : Float(256)\n      %69 : Float(256)\n      %70 : Float(256)\n      %71 : Float(256)\n      %72 : Long()\n      %73 : Float(256, 128, 1, 1)\n      %74 : Float(256)\n      %75 : Float(256)\n      %76 : Float(256)\n      %77 : Float(256)\n      %78 : Long()\n      %79 : Float(256, 256, 3, 3)\n      %80 : Float(256)\n      %81 : Float(256)\n      %82 : Float(256)\n      %83 : Float(256)\n      %84 : Long()\n      %85 : Float(256, 256, 3, 3)\n      %86 : Float(256)\n      %87 : Float(256)\n      %88 : Float(256)\n      %89 : Float(256)\n      %90 : Long()\n      %91 : Float(512, 256, 3, 3)\n      %92 : Float(512)\n      %93 : Float(512)\n      %94 : Float(512)\n      %95 : Float(512)\n      %96 : Long()\n      %97 : Float(512, 512, 3, 3)\n      %98 : Float(512)\n      %99 : Float(512)\n      %100 : Float(512)\n      %101 : Float(512)\n      %102 : Long()\n      %103 : Float(512, 256, 1, 1)\n      %104 : Float(512)\n      %105 : Float(512)\n      %106 : Float(512)\n      %107 : Float(512)\n      %108 : Long()\n      %109 : Float(512, 512, 3, 3)\n      %110 : Float(512)\n      %111 : Float(512)\n      %112 : Float(512)\n      %113 : Float(512)\n      %114 : Long()\n      %115 : Float(512, 512, 3, 3)\n      %116 : Float(512)\n      %117 : Float(512)\n      %118 : Float(512)\n      %119 : Float(512)\n      %120 : Long()\n      %121 : Float(512, 256, 4, 4)\n      %122 : Float(256)\n      %123 : Float(256)\n      %124 : Float(256)\n      %125 : Float(256)\n      %126 : Long()\n      %127 : Float(256, 256, 4, 4)\n      %128 : Float(256)\n      %129 : Float(256)\n      %130 : Float(256)\n      %131 : Float(256)\n      %132 : Long()\n      %133 : Float(256, 256, 4, 4)\n      %134 : Float(256)\n      %135 : Float(256)\n      %136 : Float(256)\n      %137 : Float(256)\n      %138 : Long()\n      %139 : Float(64, 256, 3, 3)\n      %140 : Float(64)\n      %141 : Float(9, 64, 1, 1)\n      %142 : Float(9)\n      %143 : Float(64, 256, 3, 3)\n      %144 : Float(64)\n      %145 : Float(2, 64, 1, 1)\n      %146 : Float(2)\n      %147 : Float(64, 256, 3, 3)\n      %148 : Float(64)\n      %149 : Float(2, 64, 1, 1)\n      %150 : Float(2)) {\n  %151 : Float(1, 64, 256, 256) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%0, %1), scope: PoseResNet/Conv2d[conv1]\n  %152 : Float(1, 64, 256, 256) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%151, %2, %3, %4, %5), scope: PoseResNet/BatchNorm2d[bn1]\n  %153 : Float(1, 64, 256, 256) = onnx::Relu(%152), scope: PoseResNet/ReLU[relu]\n  %154 : Float(1, 64, 128, 128) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%153), scope: PoseResNet/MaxPool2d[maxpool]\n  %155 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%154, %7), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1]\n  %156 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%155, %8, %9, %10, %11), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1]\n  %157 : Float(1, 64, 128, 128) = onnx::Relu(%156), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n  %158 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%157, %13), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2]\n  %159 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%158, %14, %15, %16, %17), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]\n  %160 : Float(1, 64, 128, 128) = onnx::Add(%159, %154), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]\n  %161 : Float(1, 64, 128, 128) = onnx::Relu(%160), scope: PoseResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n  %162 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%161, %19), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1]\n  %163 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%162, %20, %21, %22, %23), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn1]\n  %164 : Float(1, 64, 128, 128) = onnx::Relu(%163), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n  %165 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%164, %25), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv2]\n  %166 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%165, %26, %27, %28, %29), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]\n  %167 : Float(1, 64, 128, 128) = onnx::Add(%166, %161), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]\n  %168 : Float(1, 64, 128, 128) = onnx::Relu(%167), scope: PoseResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n  %169 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%168, %31), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1]\n  %170 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%169, %32, %33, %34, %35), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1]\n  %171 : Float(1, 128, 64, 64) = onnx::Relu(%170), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n  %172 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%171, %37), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2]\n  %173 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%172, %38, %39, %40, %41), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]\n  %174 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%168, %43), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n  %175 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%174, %44, %45, %46, %47), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n  %176 : Float(1, 128, 64, 64) = onnx::Add(%173, %175), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]\n  %177 : Float(1, 128, 64, 64) = onnx::Relu(%176), scope: PoseResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n  %178 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%177, %49), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv1]\n  %179 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%178, %50, %51, %52, %53), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn1]\n  %180 : Float(1, 128, 64, 64) = onnx::Relu(%179), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n  %181 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%180, %55), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv2]\n  %182 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%181, %56, %57, %58, %59), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]\n  %183 : Float(1, 128, 64, 64) = onnx::Add(%182, %177), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]\n  %184 : Float(1, 128, 64, 64) = onnx::Relu(%183), scope: PoseResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n  %185 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%184, %61), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1]\n  %186 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%185, %62, %63, %64, %65), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1]\n  %187 : Float(1, 256, 32, 32) = onnx::Relu(%186), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n  %188 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%187, %67), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2]\n  %189 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%188, %68, %69, %70, %71), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]\n  %190 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%184, %73), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n  %191 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%190, %74, %75, %76, %77), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n  %192 : Float(1, 256, 32, 32) = onnx::Add(%189, %191), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]\n  %193 : Float(1, 256, 32, 32) = onnx::Relu(%192), scope: PoseResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n  %194 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%193, %79), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv1]\n  %195 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%194, %80, %81, %82, %83), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn1]\n  %196 : Float(1, 256, 32, 32) = onnx::Relu(%195), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n  %197 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%196, %85), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv2]\n  %198 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%197, %86, %87, %88, %89), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]\n  %199 : Float(1, 256, 32, 32) = onnx::Add(%198, %193), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]\n  %200 : Float(1, 256, 32, 32) = onnx::Relu(%199), scope: PoseResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n  %201 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%200, %91), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1]\n  %202 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%201, %92, %93, %94, %95), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn1]\n  %203 : Float(1, 512, 16, 16) = onnx::Relu(%202), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n  %204 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%203, %97), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2]\n  %205 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%204, %98, %99, %100, %101), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]\n  %206 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%200, %103), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n  %207 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%206, %104, %105, %106, %107), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n  %208 : Float(1, 512, 16, 16) = onnx::Add(%205, %207), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]\n  %209 : Float(1, 512, 16, 16) = onnx::Relu(%208), scope: PoseResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n  %210 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%209, %109), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv1]\n  %211 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%210, %110, %111, %112, %113), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn1]\n  %212 : Float(1, 512, 16, 16) = onnx::Relu(%211), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n  %213 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%212, %115), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv2]\n  %214 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%213, %116, %117, %118, %119), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]\n  %215 : Float(1, 512, 16, 16) = onnx::Add(%214, %209), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]\n  %216 : Float(1, 512, 16, 16) = onnx::Relu(%215), scope: PoseResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n  %217 : Float(1, 256, 32, 32) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%216, %121), scope: PoseResNet/Sequential[deconv_layers]/ConvTranspose2d[0]\n  %218 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%217, %122, %123, %124, %125), scope: PoseResNet/Sequential[deconv_layers]/BatchNorm2d[1]\n  %219 : Float(1, 256, 32, 32) = onnx::Relu(%218), scope: PoseResNet/Sequential[deconv_layers]/ReLU[2]\n  %220 : Float(1, 256, 64, 64) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%219, %127), scope: PoseResNet/Sequential[deconv_layers]/ConvTranspose2d[3]\n  %221 : Float(1, 256, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%220, %128, %129, %130, %131), scope: PoseResNet/Sequential[deconv_layers]/BatchNorm2d[4]\n  %222 : Float(1, 256, 64, 64) = onnx::Relu(%221), scope: PoseResNet/Sequential[deconv_layers]/ReLU[5]\n  %223 : Float(1, 256, 128, 128) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%222, %133), scope: PoseResNet/Sequential[deconv_layers]/ConvTranspose2d[6]\n  %224 : Float(1, 256, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%223, %134, %135, %136, %137), scope: PoseResNet/Sequential[deconv_layers]/BatchNorm2d[7]\n  %225 : Float(1, 256, 128, 128) = onnx::Relu(%224), scope: PoseResNet/Sequential[deconv_layers]/ReLU[8]\n  %226 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%225, %139, %140), scope: PoseResNet/Sequential[hm]/Conv2d[0]\n  %227 : Float(1, 64, 128, 128) = onnx::Relu(%226), scope: PoseResNet/Sequential[hm]/ReLU[1]\n  %228 : Float(1, 9, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%227, %141, %142), scope: PoseResNet/Sequential[hm]/Conv2d[2]\n  %229 : Float(1, 9, 128, 128) = onnx::Sigmoid(%228), scope: PoseResNet/Sequential[hm]/Sigmoid[3]\n  %230 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%225, %147, %148), scope: PoseResNet/Sequential[wh]/Conv2d[0]\n  %231 : Float(1, 64, 128, 128) = onnx::Relu(%230), scope: PoseResNet/Sequential[wh]/ReLU[1]\n  %232 : Float(1, 2, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%231, %149, %150), scope: PoseResNet/Sequential[wh]/Conv2d[2]\n  %233 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%225, %143, %144), scope: PoseResNet/Sequential[reg]/Conv2d[0]\n  %234 : Float(1, 64, 128, 128) = onnx::Relu(%233), scope: PoseResNet/Sequential[reg]/ReLU[1]\n  %235 : Float(1, 2, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%234, %145, %146), scope: PoseResNet/Sequential[reg]/Conv2d[2]\n  return (%229, %232, %235);\n}\n\n"
    }
   ],
   "source": [
    "torch_model = model\n",
    "batch_size = 1\n",
    "input_shape = (3, 512, 512)\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model = torch_model\n",
    "torch_model.eval()\n",
    "\n",
    "\n",
    "dummy_input = Variable(torch.randn(1,3,512,512))\n",
    "export_onnx_file = \"/home/omnieyes/renjie/GitHub/CenterNet/exp/ctdet/omnieyes_res_18/model_best_add_sigmoid.onnx\" \n",
    "torch.onnx.export(torch_model,\n",
    "                    dummy_input,\n",
    "                    export_onnx_file,\n",
    "                    #keep_initializers_as_inputs=True,\n",
    "#                     opset_version=10,\n",
    "#                     do_constant_folding=True,\t# 是否执行常量折叠优化\n",
    "#                     input_names=[\"input\"],\t\t# 输入名\n",
    "#                     output_names=[\"output\"],\t# 输出名\n",
    "\n",
    "                    verbose=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 256, 256]           9,408\n       BatchNorm2d-2         [-1, 64, 256, 256]             128\n              ReLU-3         [-1, 64, 256, 256]               0\n         MaxPool2d-4         [-1, 64, 128, 128]               0\n            Conv2d-5         [-1, 64, 128, 128]          36,864\n       BatchNorm2d-6         [-1, 64, 128, 128]             128\n              ReLU-7         [-1, 64, 128, 128]               0\n            Conv2d-8         [-1, 64, 128, 128]          36,864\n       BatchNorm2d-9         [-1, 64, 128, 128]             128\n             ReLU-10         [-1, 64, 128, 128]               0\n       BasicBlock-11         [-1, 64, 128, 128]               0\n           Conv2d-12         [-1, 64, 128, 128]          36,864\n      BatchNorm2d-13         [-1, 64, 128, 128]             128\n             ReLU-14         [-1, 64, 128, 128]               0\n           Conv2d-15         [-1, 64, 128, 128]          36,864\n      BatchNorm2d-16         [-1, 64, 128, 128]             128\n             ReLU-17         [-1, 64, 128, 128]               0\n       BasicBlock-18         [-1, 64, 128, 128]               0\n           Conv2d-19          [-1, 128, 64, 64]          73,728\n      BatchNorm2d-20          [-1, 128, 64, 64]             256\n             ReLU-21          [-1, 128, 64, 64]               0\n           Conv2d-22          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-23          [-1, 128, 64, 64]             256\n           Conv2d-24          [-1, 128, 64, 64]           8,192\n      BatchNorm2d-25          [-1, 128, 64, 64]             256\n             ReLU-26          [-1, 128, 64, 64]               0\n       BasicBlock-27          [-1, 128, 64, 64]               0\n           Conv2d-28          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-29          [-1, 128, 64, 64]             256\n             ReLU-30          [-1, 128, 64, 64]               0\n           Conv2d-31          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-32          [-1, 128, 64, 64]             256\n             ReLU-33          [-1, 128, 64, 64]               0\n       BasicBlock-34          [-1, 128, 64, 64]               0\n           Conv2d-35          [-1, 256, 32, 32]         294,912\n      BatchNorm2d-36          [-1, 256, 32, 32]             512\n             ReLU-37          [-1, 256, 32, 32]               0\n           Conv2d-38          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-39          [-1, 256, 32, 32]             512\n           Conv2d-40          [-1, 256, 32, 32]          32,768\n      BatchNorm2d-41          [-1, 256, 32, 32]             512\n             ReLU-42          [-1, 256, 32, 32]               0\n       BasicBlock-43          [-1, 256, 32, 32]               0\n           Conv2d-44          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-45          [-1, 256, 32, 32]             512\n             ReLU-46          [-1, 256, 32, 32]               0\n           Conv2d-47          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-48          [-1, 256, 32, 32]             512\n             ReLU-49          [-1, 256, 32, 32]               0\n       BasicBlock-50          [-1, 256, 32, 32]               0\n           Conv2d-51          [-1, 512, 16, 16]       1,179,648\n      BatchNorm2d-52          [-1, 512, 16, 16]           1,024\n             ReLU-53          [-1, 512, 16, 16]               0\n           Conv2d-54          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-55          [-1, 512, 16, 16]           1,024\n           Conv2d-56          [-1, 512, 16, 16]         131,072\n      BatchNorm2d-57          [-1, 512, 16, 16]           1,024\n             ReLU-58          [-1, 512, 16, 16]               0\n       BasicBlock-59          [-1, 512, 16, 16]               0\n           Conv2d-60          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-61          [-1, 512, 16, 16]           1,024\n             ReLU-62          [-1, 512, 16, 16]               0\n           Conv2d-63          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n             ReLU-65          [-1, 512, 16, 16]               0\n       BasicBlock-66          [-1, 512, 16, 16]               0\n  ConvTranspose2d-67          [-1, 256, 32, 32]       2,097,152\n      BatchNorm2d-68          [-1, 256, 32, 32]             512\n             ReLU-69          [-1, 256, 32, 32]               0\n  ConvTranspose2d-70          [-1, 256, 64, 64]       1,048,576\n      BatchNorm2d-71          [-1, 256, 64, 64]             512\n             ReLU-72          [-1, 256, 64, 64]               0\n  ConvTranspose2d-73        [-1, 256, 128, 128]       1,048,576\n      BatchNorm2d-74        [-1, 256, 128, 128]             512\n             ReLU-75        [-1, 256, 128, 128]               0\n           Conv2d-76         [-1, 64, 128, 128]         147,520\n             ReLU-77         [-1, 64, 128, 128]               0\n           Conv2d-78          [-1, 9, 128, 128]             585\n          Sigmoid-79          [-1, 9, 128, 128]               0\n           Conv2d-80         [-1, 64, 128, 128]         147,520\n             ReLU-81         [-1, 64, 128, 128]               0\n           Conv2d-82          [-1, 2, 128, 128]             130\n           Conv2d-83         [-1, 64, 128, 128]         147,520\n             ReLU-84         [-1, 64, 128, 128]               0\n           Conv2d-85          [-1, 2, 128, 128]             130\n================================================================\nTotal params: 15,815,757\nTrainable params: 15,815,757\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 3.00\nForward/backward pass size (MB): 504.75\nParams size (MB): 60.33\nEstimated Total Size (MB): 568.08\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(model.cuda(), (3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit",
   "language": "python",
   "name": "python361064bitf26cb2b0b83e40239d9cd18c718317ae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}